{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import pickle\n",
    "train = False\n",
    "create_pickle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    data = pd.read_csv('data/train.csv')\n",
    "else: \n",
    "    data = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train and create_pickle:\n",
    "    # Group data by 'prop_id' and calculate mean values\n",
    "    groupby_hotel = data.groupby('prop_id').agg({'position': 'mean', 'click_bool': 'mean', 'booking_bool': 'mean'})\n",
    "    \n",
    "    # Create a dictionary for hotel properties\n",
    "    dict_hotel_pos = groupby_hotel.to_dict('index')\n",
    "    \n",
    "    # Save dictionary to file\n",
    "    with open('dict_hotel_pos.pickle', 'wb') as handle:\n",
    "        pickle.dump(dict_hotel_pos, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    # Load dictionary from file\n",
    "    with open('dict_hotel_pos.pickle', 'rb') as handle:\n",
    "        dict_hotel_pos = pickle.load(handle)\n",
    "\n",
    "# Create new columns in the data and assign values from the dictionary\n",
    "data['mean_position'] = data['prop_id'].map(lambda x: dict_hotel_pos[x]['position'] if x in dict_hotel_pos else np.nan)\n",
    "data['perc_clicked'] = data['prop_id'].map(lambda x: dict_hotel_pos[x]['click_bool'] if x in dict_hotel_pos else np.nan)\n",
    "data['perc_booked'] = data['prop_id'].map(lambda x: dict_hotel_pos[x]['booking_bool'] if x in dict_hotel_pos else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    data['target'] = np.where(data['click_bool'] == 1, 1, 0)\n",
    "    data['target'] = np.where(data['booking_bool'] == 1, 5, data['target'])\t\n",
    "    data = data.drop(['click_bool', 'booking_bool'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one feature of comp rate and comp_perc_diff for every competitor\n",
    "for i in range(1,9):\n",
    "    data['comp_rate_' + str(i)] = data['comp' + str(i) + '_rate'] * data['comp' + str(i) + '_rate_percent_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform integers to strings for categorical data\n",
    "data['srch_destination_id'] = data['srch_destination_id'].astype(str)\n",
    "data['site_id'] = data['site_id'].astype(str)\n",
    "data['visitor_location_country_id'] = data['visitor_location_country_id'].astype(str)\n",
    "data['prop_country_id'] = data['prop_country_id'].astype(str)\n",
    "data['prop_id'] = data['prop_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform integers to boleans\n",
    "data[\"prop_brand_bool\"] = data[\"prop_brand_bool\"].astype(bool)\n",
    "data[\"srch_saturday_night_bool\"] = data[\"srch_saturday_night_bool\"].astype(bool)\n",
    "data[\"random_bool\"] = data[\"random_bool\"].astype(bool)\n",
    "data[\"promotion_flag\"] = data[\"promotion_flag\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all integers to floats to compute means and z-scores\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'int64':\n",
    "        data[col] = data[col].astype(float)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the columns prop_starrating and prop_review_score, a 0 represents a missing value and should be replaced by nan\n",
    "data['prop_starrating'] = data['prop_starrating'].replace(0, np.nan)\n",
    "data['prop_review_score'] = data['prop_review_score'].replace(0, np.nan)\n",
    "data['srch_query_affinity_score'] = data['srch_query_affinity_score'].replace(0, np.nan)\n",
    "data[\"prop_log_historical_price\"] = data[\"prop_log_historical_price\"].replace(0, np.nan)\n",
    "data[\"orig_destination_distance\"] = data[\"orig_destination_distance\"].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename date time column to month (january, february, march, april, may, june, july, august, september, october, november, december)\n",
    "data['date_time'] = pd.to_datetime(data['date_time'])\n",
    "data['date_time'] = data['date_time'].dt.month_name()\n",
    "\n",
    "# rename date_time column name to month\n",
    "data = data.rename(columns={'date_time': 'month'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANNOT USE THIS FEATURE, TOO MANY MISSING VALUES:\n",
    "# visitor_hist_starrating 0.9488966226896648 nan\n",
    "# visitor_hist_adr_usd 0.9486560588709875 nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add feature that calculates how much the hotel rating is above the average rating of the hotels in the same region \n",
    "data['rating_relto_region'] = data.groupby('srch_destination_id')['prop_starrating'].transform(lambda x: x - x.mean())\n",
    "\n",
    "# add feature that indicates how much the hotels rating is in same price range\n",
    "data['rating_relto_search'] = data.groupby('srch_id')['prop_starrating'].transform(lambda x: x - x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo check if review score is above average of other search results\n",
    "data['review_relto_search'] = data.groupby('srch_id')['prop_review_score'].transform(lambda x: x - x.mean())\n",
    "data['review_relto_region'] = data.groupby('srch_destination_id')['prop_review_score'].transform(lambda x: x - x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create price per night column\n",
    "data['price_per_night'] = data['price_usd']/data['srch_length_of_stay']\n",
    "\n",
    "# add feature that indicates if the price per night is higher than the average price per night in that region in that month\n",
    "data['price_rel_to_region'] = data['price_per_night'] - data.groupby(['month', 'srch_destination_id'])['price_per_night'].transform('mean')\n",
    "\n",
    "# add feature that indicates if the price per night is higher than the average price per night of the search id  \n",
    "data['price_rel_to_search'] = data['price_per_night'] - data.groupby('srch_id')['price_per_night'].transform('mean')\n",
    "\n",
    "# TODO: check output\n",
    "print(data['price_rel_to_region'].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the hotel is more close to the average distance of the hotels in the same region\n",
    "data['rel_distance'] = data['orig_destination_distance'] - data.groupby('srch_destination_id')['orig_destination_distance'].transform('mean')\n",
    "\n",
    "# TODO: check if this works\n",
    "print(data['rel_distance'].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean of the two columns prop_location_score1 and prop_location_score2 \n",
    "data['prop_location_score2'] = data['prop_location_score2'].fillna(data['prop_location_score1'])\n",
    "data['prop_location_score1'] = data['prop_location_score1'].fillna(data['prop_location_score2'])\n",
    "data['prop_location_score'] = data[['prop_location_score2', 'prop_location_score1']].mean(axis=1)\n",
    "data = data.drop(['prop_location_score1', 'prop_location_score2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_rate mean is the mean of all not nan values in all columns that contain comb_rate\n",
    "data['comb_rate'] = data[['comp1_rate', 'comp2_rate', 'comp3_rate', 'comp4_rate', 'comp5_rate', 'comp6_rate', 'comp7_rate', 'comp8_rate']].mean(axis=1, skipna=True)\n",
    "data[\"comb_inv\"] = data[['comp1_inv', 'comp2_inv', 'comp3_inv', 'comp4_inv', 'comp5_inv', 'comp6_inv', 'comp7_inv', 'comp8_inv']].sum(axis=1, skipna=True)\n",
    "print(data['comb_inv'].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['comp1_rate', 'comp2_rate', 'comp3_rate', 'comp4_rate', 'comp5_rate', 'comp6_rate', 'comp7_rate', 'comp8_rate'], axis=1)\n",
    "data = data.drop(['comp1_rate_percent_diff', 'comp2_rate_percent_diff', 'comp3_rate_percent_diff', 'comp4_rate_percent_diff', 'comp5_rate_percent_diff', 'comp6_rate_percent_diff', 'comp7_rate_percent_diff', 'comp8_rate_percent_diff'], axis=1)\n",
    "data = data.drop(['comp1_inv', 'comp2_inv', 'comp3_inv', 'comp4_inv', 'comp5_inv', 'comp6_inv', 'comp7_inv', 'comp8_inv'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to latex table\n",
    "import os\n",
    "if not os.path.exists('latex'):\n",
    "    os.makedirs('latex')\n",
    "\n",
    "# create a table for all numerical data with mean, median, highest, lowest and standard deviation\n",
    "num_describtion = data.describe() \n",
    "num_describtion.to_latex('latex/num_describtion.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of unique categories, most occuring, least occuring and percentage of most occuring category for categorical data\n",
    "dict_cat = {}\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        unique = data[col].nunique()\n",
    "        moc = data[col].value_counts().idxmax()\n",
    "        moc_freq = data[col].value_counts().max()/data.shape[0]\n",
    "        loc = data[col].value_counts().idxmin()\n",
    "        loc_freq = data[col].value_counts().min()/data.shape[0]\n",
    "        \n",
    "        dict_cat[col] = [unique, moc, moc_freq, loc, loc_freq]\n",
    "    \n",
    "# create table for categorical data\n",
    "cat_describtion = pd.DataFrame.from_dict(dict_cat, orient='index', columns=['unique', 'most occuring', 'most occuring frequency', 'least occuring', 'least occuring frequency'])\n",
    "\n",
    "# save to latex\n",
    "cat_describtion.to_latex('cat_describtion.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print missing values in each column\n",
    "for col in data.columns:\n",
    "    # print percentage of missing values\n",
    "    print(col, data[col].isnull().sum()/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns that contain more than 40% missing values\n",
    "for col in data.columns:\n",
    "    if data[col].isnull().sum()/data.shape[0] > 0.40:\n",
    "        print(col, data[col].isnull().sum()/data.shape[0])\n",
    "        data = data.drop([col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # handle missing values \n",
    "# for col in data.columns:\n",
    "    \n",
    "#     # check if numerical\n",
    "#     if data[col].dtype == 'float64' or data[col].dtype == 'int64':   \n",
    "#         data[col] = data[col].fillna(data[col].mean())\n",
    "        \n",
    "#         # TODO: not always mean, sometimes median or mode\n",
    "    \n",
    "#     # check if categorical\n",
    "#     if data[col].dtype == 'object' or data[col].dtype == 'bool':\n",
    "#         data[col] = data[col].fillna(data[col].value_counts().idxmax())\n",
    "\n",
    "# # check if there are still missing values\n",
    "# print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove the columns ... and ... that are not in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print amount of unique values in each categorical column\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object' or data[col].dtype == 'bool':\n",
    "        print(col, data[col].nunique())\n",
    "        \n",
    "# drop srch_destination_id because it has too many unique values\n",
    "data = data.drop(['srch_destination_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle bins with less than 5 occurences in  site_id, visitor_location_country_id, prop_country_id\n",
    "for col in ['site_id', 'visitor_location_country_id', 'prop_country_id']:\n",
    "    \n",
    "    # merge bins with less than 5 occurences with the bin with the most occurences\n",
    "    for sub_col in data[col].unique():\n",
    "        if data[col].value_counts()[sub_col] < 5:\n",
    "            \n",
    "            # merge them into 'other' category\n",
    "            data[col] = data[col].replace(sub_col, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are still missing values\n",
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def fill_missing_with_regression(df, target_column, feature_columns):\n",
    "    # Split the data into two sets: one with missing values and one without\n",
    "    df_missing = df[df[target_column].isnull()]\n",
    "    df_not_missing = df[df[target_column].notnull()]\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_not_missing.loc[:, feature_columns] = imputer.fit_transform(df_not_missing.loc[:, feature_columns])\n",
    "\n",
    "    # Prepare the feature matrix and target vector for the regression\n",
    "    X_train = df_not_missing[feature_columns]\n",
    "    y_train = df_not_missing[target_column]\n",
    "    X_test = df_missing[feature_columns]\n",
    "    X_test = imputer.transform(X_test)\n",
    "    \n",
    "    # Create and fit the linear regression model\n",
    "    regression_model = LinearRegression()\n",
    "    regression_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the missing values\n",
    "    y_pred = regression_model.predict(X_test)\n",
    "\n",
    "    # Fill in the missing values in the DataFrame\n",
    "    df.loc[df[target_column].isnull(), target_column] = y_pred\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def fill_missing_with_clustering(df, target_column, feature_columns, n_clusters):\n",
    "    # Split the data into two sets: one with missing values and one without\n",
    "    df_missing = df[df[target_column].isnull()]\n",
    "    df_not_missing = df[df[target_column].notnull()]\n",
    "\n",
    "    # Prepare the feature matrix for clustering\n",
    "    X_train = df_not_missing[feature_columns]\n",
    "\n",
    "    # Create and fit the KMeans clustering model\n",
    "    clustering_model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clustering_model.fit(X_train)\n",
    "\n",
    "    # Assign cluster labels to non-missing values\n",
    "    df_not_missing['cluster_label'] = clustering_model.labels_\n",
    "\n",
    "    # Prepare the feature matrix for missing values\n",
    "    X_test = df_missing[feature_columns]\n",
    "\n",
    "    # Predict the cluster labels for missing values\n",
    "    cluster_labels = clustering_model.predict(X_test)\n",
    "\n",
    "    # Find the most frequent category for each cluster\n",
    "    cluster_mode_values = df_not_missing.groupby('cluster_label')[target_column].apply(lambda x: x.mode().iloc[0])\n",
    "\n",
    "    # Fill in the missing values based on cluster labels\n",
    "    df.loc[df[target_column].isnull(), target_column] = cluster_labels.map(cluster_mode_values)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing values with regression model\n",
    "num_columns = data.select_dtypes(include=['float64', 'int64']).columns\t\n",
    "\n",
    "for col in data.columns:\n",
    "    \n",
    "    # check whether column has missing values\n",
    "    if data[col].isnull().sum() != 0:\n",
    "    \n",
    "        if data[col].dtype == 'float64' or data[col].dtype == 'int64':\n",
    "            print(col)\n",
    "            # fill in missing values with regression model\n",
    "            data = fill_missing_with_regression(data, col, num_columns)\n",
    "            \n",
    "        else:\n",
    "            # fill in missing values with clustering model\n",
    "            data = fill_missing_with_clustering(data, col, num_columns, 5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are still missing values\n",
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data in data folder\n",
    "if train:\n",
    "    data.to_csv('data/train_cleaned.csv', index=False)\n",
    "else:\n",
    "    data.to_csv('data/test_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
